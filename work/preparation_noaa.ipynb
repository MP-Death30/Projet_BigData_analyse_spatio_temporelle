{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494b1670-458c-4574-bb7d-7922f7fa98c9",
   "metadata": {},
   "source": [
    "Import des librairies nécessaires à la préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b9f4fa0-c8c6-44d4-8814-92f0179c026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ef04c-ce57-466a-88c0-7ac8fb7168cb",
   "metadata": {},
   "source": [
    "Récupération du fichier avec la correspondance id_station et coordonnées GPS pour ne garder que les fichiers des stations près de NYC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f0e44-3643-436f-9b70-d3f01791295c",
   "metadata": {},
   "source": [
    "Phase 1 : Initialisation de l'Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d6045c9-0264-4370-95a1-2268f3da9fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Spark initialisée avec les configurations de robustesse.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "# --- 1. Configuration de Robustesse et Initialisation Spark ---\n",
    "# Augmentation des timeouts et allocation de mémoire stricte pour éviter les crashs JVM\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataLake_NOAA_NYC_Prep\") \\\n",
    "    .config(\"spark.executor.memory\", \"3g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.rpc.askTimeout\", \"800s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# --- 2. Définition des Paramètres Géographiques et HDFS ---\n",
    "# Boîte englobante de la région de NYC\n",
    "MIN_LAT, MAX_LAT = 40.0, 41.5\n",
    "MIN_LON, MAX_LON = -75.0, -73.0\n",
    "\n",
    "# Chemin HDFS BRUT (Corrigé avec l'hôte et le port du namenode)\n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "\n",
    "# Plage d'années\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "print(\"Session Spark initialisée avec les configurations de robustesse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29e526-c998-400d-9d18-45236d8b465c",
   "metadata": {},
   "source": [
    "Phase 2 : Métadonnées et Identification des Stations NOAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6ad18d-ae5c-4807-95eb-3b54176a858a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 93 Stations NOAA pertinentes trouvées près de New York.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Téléchargement des Métadonnées des Stations (via Pandas car petit fichier) ---\n",
    "stations_url = \"https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv\"\n",
    "pdf_stations = pd.read_csv(stations_url,\n",
    "                         dtype={'USAF': str, 'WBAN': str})\n",
    "\n",
    "pdf_stations['STN_ID'] = pdf_stations['USAF'].str.strip() + pdf_stations['WBAN'].str.strip()\n",
    "pdf_stations = pdf_stations.rename(columns={'LAT': 'LATITUDE', 'LON': 'LONGITUDE'})\n",
    "pdf_stations = pdf_stations.dropna(subset=['LATITUDE', 'LONGITUDE', 'STATION NAME'])\n",
    "spark_stations_df = spark.createDataFrame(pdf_stations)\n",
    "\n",
    "# --- 2. Filtrage Géographique ---\n",
    "nyc_stations_spark = spark_stations_df.filter(\n",
    "    (F.col('LATITUDE') >= MIN_LAT) & (F.col('LATITUDE') <= MAX_LAT) &\n",
    "    (F.col('LONGITUDE') >= MIN_LON) & (F.col('LONGITUDE') <= MAX_LON)\n",
    ")\n",
    "\n",
    "# Récupération de la liste des IDs pertinents (pour filtrage par nom de fichier)\n",
    "relevant_station_ids = [row.STN_ID for row in nyc_stations_spark.select(\"STN_ID\").collect()]\n",
    "\n",
    "print(f\"\\n✅ {nyc_stations_spark.count()} Stations NOAA pertinentes trouvées près de New York.\")\n",
    "# Gardons ce DataFrame pour la jointure des coordonnées plus tard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21de6ad-c870-438e-bd32-0eb6f005656f",
   "metadata": {},
   "source": [
    "Phase 2 bis : Téléchargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbd338ec-5fac-406b-b377-aed46fa0e614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage du téléchargement pour 93 stations de 2005 à 2023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Téléchargement des fichiers GSOD:  31%|███       | 552/1767 [22:41<49:56,  2.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Téléchargement terminé. 552 fichiers GSOD traités (téléchargés ou existants).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm # Importation de tqdm pour une barre de progression\n",
    "\n",
    "# --- Paramètres de Configuration ---\n",
    "BASE_URL = \"https://www.ncei.noaa.gov/data/global-summary-of-the-day/access\"\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/noaa_gsod\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "# --- IDs des stations à télécharger ---\n",
    "# NOTE: Cette liste doit être remplie avec le résultat de votre Phase 2 !\n",
    "# Exemple de données de test si la Phase 2 n'est pas exécutée :\n",
    "# relevant_station_ids = [\"72503094728\", \"72503014732\"] \n",
    "\n",
    "# Si vous exécutez ce bloc après la Phase 2, assurez-vous que la liste est disponible.\n",
    "if 'relevant_station_ids' not in locals():\n",
    "    print(\"⚠️ ATTENTION: La liste 'relevant_station_ids' n'est pas définie. Veuillez exécuter la Phase 2 en premier.\")\n",
    "    # On sort du script si la liste n'est pas disponible pour éviter de télécharger inutilement\n",
    "    exit()\n",
    "\n",
    "# Démarrage du processus de téléchargement\n",
    "print(f\"Démarrage du téléchargement pour {len(relevant_station_ids)} stations de {START_YEAR} à {END_YEAR}.\")\n",
    "\n",
    "total_files = len(relevant_station_ids) * (END_YEAR - START_YEAR + 1)\n",
    "downloaded_count = 0\n",
    "\n",
    "# Utilisation de tqdm pour la barre de progression\n",
    "with tqdm(total=total_files, desc=\"Téléchargement des fichiers GSOD\") as pbar:\n",
    "    \n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        year_dir = os.path.join(LOCAL_BASE_DIR, str(year))\n",
    "        \n",
    "        # Crée le répertoire de l'année s'il n'existe pas\n",
    "        os.makedirs(year_dir, exist_ok=True)\n",
    "        \n",
    "        for station_id in relevant_station_ids:\n",
    "            file_name = f\"{station_id}.csv\"\n",
    "            local_path = os.path.join(year_dir, file_name)\n",
    "            remote_url = f\"{BASE_URL}/{year}/{file_name}\"\n",
    "\n",
    "            # Vérifie si le fichier existe déjà pour éviter de le re-télécharger\n",
    "            if os.path.exists(local_path):\n",
    "                # print(f\"Fichier existant: {local_path}. Ignoré.\")\n",
    "                pbar.update(1)\n",
    "                downloaded_count += 1\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Requête HTTP GET pour télécharger le fichier\n",
    "                response = requests.get(remote_url, timeout=10)\n",
    "                response.raise_for_status() # Lève une exception si le statut HTTP est 4xx ou 5xx\n",
    "                \n",
    "                # Écrit le contenu dans le fichier local\n",
    "                with open(local_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                downloaded_count += 1\n",
    "                pbar.update(1)\n",
    "                # Pause pour être poli avec le serveur NOAA\n",
    "                time.sleep(0.1) \n",
    "                \n",
    "            except requests.exceptions.HTTPError as errh:\n",
    "                # Fichier 404/Not Found (la station n'a pas forcément de données pour cette année)\n",
    "                if response.status_code == 404:\n",
    "                    pass # On ignore simplement ce fichier manquant\n",
    "                else:\n",
    "                    print(f\"\\n❌ Erreur HTTP pour {remote_url}: {errh}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"\\n❌ Erreur de Connexion/Timeout pour {remote_url}: {e}\")\n",
    "\n",
    "print(f\"\\n✅ Téléchargement terminé. {downloaded_count} fichiers GSOD traités (téléchargés ou existants).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbb709-9985-4754-8f81-3724928ef9cb",
   "metadata": {},
   "source": [
    "Phase 3 : Ingestion Ciblée et Persistance (Couche Raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a98c2825-e1c6-450b-a5dd-40d475bcfa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de 552 fichiers existants seront lus par Spark.\n",
      "\n",
      "Sauvegarde de la copie BRUTE filtrée (2005-2023) dans : hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet...\n",
      "✅ Copie brute sauvegardée sur HDFS. Le traitement peut se poursuivre.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration des Chemins ---\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/noaa_gsod\" \n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "START_YEAR = 2005\n",
    "END_YEAR = 2023\n",
    "\n",
    "# --- 1. Définition des Chemins Ciblés ---\n",
    "# Nous recréons la liste, mais cette fois en utilisant 'glob' ou une vérification OS\n",
    "# pour ne pas inclure les chemins qui n'existent pas.\n",
    "\n",
    "existing_targeted_paths = []\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    for station_id in relevant_station_ids:\n",
    "        # Chemin absolu corrigé : /home/jovyan/work/data/noaa_gsod/2005/XXXXX.csv\n",
    "        path = f\"{LOCAL_BASE_DIR}/{year}/{station_id}.csv\"\n",
    "        \n",
    "        # Vérifie si le fichier existe vraiment avant de l'ajouter à la liste de lecture de Spark\n",
    "        if os.path.exists(path):\n",
    "            existing_targeted_paths.append(path)\n",
    "\n",
    "# Si aucun chemin n'existe, nous aurons une erreur, mais au moins nous savons pourquoi.\n",
    "if not existing_targeted_paths:\n",
    "    raise FileNotFoundError(\"Aucun fichier GSOD cible n'a été trouvé dans le répertoire local.\")\n",
    "\n",
    "gsod_data_paths = existing_targeted_paths\n",
    "print(f\"Total de {len(gsod_data_paths)} fichiers existants seront lus par Spark.\")\n",
    "\n",
    "# --- 2. Schéma et Lecture ---\n",
    "gsod_schema = StructType([\n",
    "    StructField(\"STATION\", StringType(), True),\n",
    "    StructField(\"DATE\", StringType(), True),\n",
    "    StructField(\"LATITUDE\", DoubleType(), True), \n",
    "    StructField(\"LONGITUDE\", DoubleType(), True),\n",
    "    StructField(\"ELEVATION\", DoubleType(), True),\n",
    "    StructField(\"NAME\", StringType(), True),\n",
    "    StructField(\"TEMP\", DoubleType(), True),\n",
    "    StructField(\"TEMP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"DEWP\", DoubleType(), True),\n",
    "    StructField(\"DEWP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"SLP\", DoubleType(), True),\n",
    "    StructField(\"SLP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"STP\", DoubleType(), True),\n",
    "    StructField(\"STP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"VISIB\", DoubleType(), True),\n",
    "    StructField(\"VISIB_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"WDSP\", DoubleType(), True),\n",
    "    StructField(\"WDSP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"MXSPD\", DoubleType(), True),\n",
    "    StructField(\"GUST\", DoubleType(), True),\n",
    "    StructField(\"MAX\", DoubleType(), True),\n",
    "    StructField(\"MAX_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"MIN\", DoubleType(), True),\n",
    "    StructField(\"MIN_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"PRCP\", DoubleType(), True),\n",
    "    StructField(\"PRCP_ATTRIBUTES\", StringType(), True),\n",
    "    StructField(\"SNDP\", DoubleType(), True),\n",
    "    StructField(\"FRSHHT\", StringType(), True),\n",
    "])\n",
    "\n",
    "# Lecture distribuée des données GSOD (seulement les fichiers ciblés)\n",
    "all_gsod_data = spark.read.csv(\n",
    "    gsod_data_paths,\n",
    "    header=True,\n",
    "    schema=gsod_schema,\n",
    "    sep=','\n",
    ")\n",
    "\n",
    "# Renommage de la colonne ID\n",
    "nyc_gsod_data = all_gsod_data.withColumnRenamed(\"STATION\", \"ID_STATION\")\n",
    "\n",
    "\n",
    "# --- 3. Persistance de la Couche Brute sur HDFS ---\n",
    "print(f\"\\nSauvegarde de la copie BRUTE filtrée (2005-2023) dans : {RAW_OUTPUT_PATH}...\")\n",
    "# Cette étape transfère les données du disque local du conteneur vers HDFS\n",
    "nyc_gsod_data.write.mode(\"overwrite\").parquet(RAW_OUTPUT_PATH)\n",
    "print(\"✅ Copie brute sauvegardée sur HDFS. Le traitement peut se poursuivre.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c4c89-1bbb-4ded-b71d-cdd10ad0b18c",
   "metadata": {},
   "source": [
    "Phase 4 : Nettoyage et Jointure des Coordonnées (ETL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fbff00b-d1ef-4cd7-8f69-e8b51dea4693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Données NOAA nettoyées et enrichies des coordonnées des stations (Couche ETL Complète).\n",
      "root\n",
      " |-- ID_STATION: string (nullable = true)\n",
      " |-- DATE_OBSERVATION: string (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP_MOYENNE_F: double (nullable = true)\n",
      " |-- TEMP_ATTRIBUTES: string (nullable = true)\n",
      " |-- POINT_ROSEE_F: double (nullable = true)\n",
      " |-- DEWP_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRESSION_ATM_MER: double (nullable = true)\n",
      " |-- SLP_ATTRIBUTES: string (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- STP_ATTRIBUTES: string (nullable = true)\n",
      " |-- VISIBILITE_MILLES: double (nullable = true)\n",
      " |-- VISIB_ATTRIBUTES: string (nullable = true)\n",
      " |-- VITESSE_VENT_NOEUDS: double (nullable = true)\n",
      " |-- WDSP_ATTRIBUTES: string (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- TEMP_MAX_F: double (nullable = true)\n",
      " |-- MAX_ATTRIBUTES: string (nullable = true)\n",
      " |-- TEMP_MIN_F: double (nullable = true)\n",
      " |-- MIN_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRECIPITATION_POUCES: double (nullable = true)\n",
      " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- PHENOMENES: string (nullable = true)\n",
      " |-- LATITUDE_STATION: double (nullable = true)\n",
      " |-- LONGITUDE_STATION: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# --- Chemins de configuration ---\n",
    "RAW_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_raw_2005_2023.parquet\"\n",
    "# nyc_stations_spark doit être le DataFrame des stations filtrées de la Phase 2\n",
    "\n",
    "# Lecture du fichier Parquet depuis HDFS pour la couche de Traitement/Nettoyage (ETL)\n",
    "data_for_cleaning = spark.read.parquet(RAW_OUTPUT_PATH)\n",
    "\n",
    "# --- 1. Nettoyage et Renommage des Colonnes ---\n",
    "renaming_map = {\n",
    "    \"DATE\": \"DATE_OBSERVATION\",\n",
    "    \"TEMP\": \"TEMP_MOYENNE_F\",\n",
    "    \"DEWP\": \"POINT_ROSEE_F\",\n",
    "    \"PRCP\": \"PRECIPITATION_POUCES\",\n",
    "    \"MIN\": \"TEMP_MIN_F\",\n",
    "    \"MAX\": \"TEMP_MAX_F\",\n",
    "    \"WDSP\": \"VITESSE_VENT_NOEUDS\",\n",
    "    \"VISIB\": \"VISIBILITE_MILLES\",\n",
    "    \"SLP\": \"PRESSION_ATM_MER\",\n",
    "    \"FRSHHT\": \"PHENOMENES\"\n",
    "}\n",
    "\n",
    "gsod_renamed_df = data_for_cleaning\n",
    "for old_name, new_name in renaming_map.items():\n",
    "    if old_name in gsod_renamed_df.columns:\n",
    "        gsod_renamed_df = gsod_renamed_df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "\n",
    "# --- 2. Jointure des Coordonnées des Stations ---\n",
    "# Ajoute la LATITUDE_STATION et LONGITUDE_STATION pour le calcul de distance\n",
    "clean_final_gsod_df = gsod_renamed_df.join(\n",
    "    nyc_stations_spark.select(\n",
    "        'STN_ID', \n",
    "        F.col('LATITUDE').alias('LATITUDE_STATION'), \n",
    "        F.col('LONGITUDE').alias('LONGITUDE_STATION')\n",
    "    ), \n",
    "    on=[F.col(\"ID_STATION\") == F.col(\"STN_ID\")],\n",
    "    how='left'\n",
    ").drop(\"STN_ID\").drop(\"LATITUDE\").drop(\"LONGITUDE\") # Supprime les colonnes brutes des coordonnées\n",
    "\n",
    "\n",
    "print(\"\\nDonnées NOAA nettoyées et enrichies des coordonnées des stations (Couche ETL Complète).\")\n",
    "clean_final_gsod_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4e0e7-f52e-47a2-886d-e7e2df0e82c7",
   "metadata": {},
   "source": [
    "Phase 5 : Préparation des Coordonnées des Zones de Qualité de l'Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57332368-11f2-4340-880e-e64438374d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "43 zones de qualité de l'air (GeoJoin ID) préparées avec leurs coordonnées.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Téléchargement et Extraction du GeoJSON ---\n",
    "geojson_url = \"https://raw.githubusercontent.com/nycehs/NYC_geography/master/UHF42.geo.json\"\n",
    "\n",
    "response = requests.get(geojson_url)\n",
    "geo_data_raw = response.json()\n",
    "\n",
    "geo_records = []\n",
    "for feature in geo_data_raw['features']:\n",
    "    properties = feature['properties']\n",
    "    geometry = feature['geometry']\n",
    "    coords = geometry['coordinates']\n",
    "    \n",
    "    try:\n",
    "        # Approximation du centroïde pour la distance\n",
    "        if geometry['type'] == 'Polygon':\n",
    "            lon = coords[0][0][0]\n",
    "            lat = coords[0][0][1]\n",
    "        elif geometry['type'] == 'MultiPolygon':\n",
    "            lon = coords[0][0][0][0]\n",
    "            lat = coords[0][0][0][1]\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        geo_records.append({\n",
    "            \"GEOJOIN_ID\": properties['GEOCODE'], \n",
    "            \"LATITUDE_ZONE\": lat,\n",
    "            \"LONGITUDE_ZONE\": lon\n",
    "        })\n",
    "    except (IndexError, TypeError):\n",
    "        continue\n",
    "\n",
    "# --- 2. Création du DataFrame Spark des Coordonnées des Zones ---\n",
    "geo_schema = StructType([\n",
    "    StructField(\"GEOJOIN_ID\", StringType(), False),\n",
    "    StructField(\"LATITUDE_ZONE\", DoubleType(), True),\n",
    "    StructField(\"LONGITUDE_ZONE\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "geo_df = spark.createDataFrame(geo_records, schema=geo_schema)\n",
    "\n",
    "print(f\"\\n{geo_df.count()} zones de qualité de l'air (GeoJoin ID) préparées avec leurs coordonnées.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8569e1-fa8b-4b64-8861-9643bf5c66b1",
   "metadata": {},
   "source": [
    "Phase 6 : Calcul de la Distance (Haversine) et Voisin le Plus Proche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aa5b600-9b82-46af-954f-f6a5f2377bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Table de correspondance (Voisin le plus proche) créée :\n",
      "+----------+-----------+------------------+\n",
      "|GEOJOIN_ID|ID_STATION |DISTANCE_KM       |\n",
      "+----------+-----------+------------------+\n",
      "|0         |74486094789|5.760247595051896 |\n",
      "|101       |72503014732|14.074131616303024|\n",
      "|102       |99728099999|9.640847117034312 |\n",
      "|103       |99728099999|11.103908014210129|\n",
      "|104       |72503014732|4.739471712784995 |\n",
      "+----------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Constante du rayon de la Terre en kilomètres (km)\n",
    "R = 6371.0\n",
    "\n",
    "# Formule de la Haversine (UDF)\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    # Conversion de degrés en radians\n",
    "    lon1_rad, lat1_rad, lon2_rad, lat2_rad = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "\n",
    "    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Enregistrement de la fonction comme une UDF (User Defined Function) pour Spark\n",
    "haversine_udf = F.udf(haversine, DoubleType())\n",
    "\n",
    "# --- 1. Préparation des Coordonnées des Stations Uniques ---\n",
    "stations_coords_df = clean_final_gsod_df.select(\n",
    "    \"ID_STATION\", \"LATITUDE_STATION\", \"LONGITUDE_STATION\"\n",
    ").distinct()\n",
    "\n",
    "# --- 2. Jointure Cartésienne (Toutes les zones vs. Toutes les stations) ---\n",
    "cross_joined_df = geo_df.crossJoin(stations_coords_df)\n",
    "\n",
    "\n",
    "# --- 3. Calcul de la Distance pour chaque paire ---\n",
    "distance_df = cross_joined_df.withColumn(\n",
    "    \"DISTANCE_KM\",\n",
    "    haversine_udf(\n",
    "        F.col(\"LONGITUDE_ZONE\"), \n",
    "        F.col(\"LATITUDE_ZONE\"), \n",
    "        F.col(\"LONGITUDE_STATION\"), \n",
    "        F.col(\"LATITUDE_STATION\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 4. Identification du Voisin le Plus Proche ---\n",
    "# Trouve la ligne (station) avec la distance minimale pour chaque GEOJOIN_ID\n",
    "window_spec = Window.partitionBy(\"GEOJOIN_ID\").orderBy(F.col(\"DISTANCE_KM\"))\n",
    "\n",
    "nearest_station_df = distance_df.withColumn(\n",
    "    \"rank\", \n",
    "    F.rank().over(window_spec)\n",
    ").filter(F.col(\"rank\") == 1).drop(\"rank\")\n",
    "\n",
    "\n",
    "print(\"\\n✅ Table de correspondance (Voisin le plus proche) créée :\")\n",
    "nearest_station_df.select(\n",
    "    \"GEOJOIN_ID\", \n",
    "    \"ID_STATION\", \n",
    "    \"DISTANCE_KM\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb9645-f14c-4e47-a2b1-6fa6b78e1d55",
   "metadata": {},
   "source": [
    "Phase 7 : Jointure Finale et Persistance (Couche Insight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f91da9d-f8dc-452d-a43c-7efd1e98a01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structure du DataFrame Final (Météo attribuée à chaque zone de qualité de l'air) :\n",
      "root\n",
      " |-- GEOJOIN_ID: string (nullable = false)\n",
      " |-- DATE_OBSERVATION: string (nullable = true)\n",
      " |-- TEMP_MOYENNE_F: double (nullable = true)\n",
      " |-- PRECIPITATION_POUCES: double (nullable = true)\n",
      " |-- VITESSE_VENT_NOEUDS: double (nullable = true)\n",
      " |-- PHENOMENES: string (nullable = true)\n",
      " |-- ID_STATION: string (nullable = true)\n",
      " |-- DISTANCE_KM: double (nullable = true)\n",
      " |-- LATITUDE_ZONE: double (nullable = true)\n",
      " |-- LONGITUDE_ZONE: double (nullable = true)\n",
      "\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+-----------+------------------+-------------+--------------+\n",
      "|GEOJOIN_ID|DATE_OBSERVATION|TEMP_MOYENNE_F|PRECIPITATION_POUCES|VITESSE_VENT_NOEUDS|PHENOMENES| ID_STATION|       DISTANCE_KM|LATITUDE_ZONE|LONGITUDE_ZONE|\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+-----------+------------------+-------------+--------------+\n",
      "|       405|      2008-01-01|          41.9|                0.15|                9.9|    010000|72503014732| 8.400119154879857|    40.715308|    -73.826379|\n",
      "|       402|      2008-01-01|          41.9|                0.15|                9.9|    010000|72503014732| 3.757173548253043|    40.765055|     -73.83936|\n",
      "|       401|      2008-01-01|          41.9|                0.15|                9.9|    010000|72503014732|1.8621544795208718|    40.780124|    -73.902066|\n",
      "|       107|      2008-01-01|          41.9|                0.15|                9.9|    010000|72503014732| 6.161877294189115|    40.833646|    -73.892155|\n",
      "|       106|      2008-01-01|          41.9|                0.15|                9.9|    010000|72503014732| 8.039508612144148|    40.846455|    -73.914385|\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+-----------+------------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Sauvegarde du DataFrame final Nettoyé/Enrichi (Couche Insight) dans : hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_enriched_clean.parquet...\n",
      "✅ Jointure finale et persistance terminées. Le jeu de données météo est prêt pour l'analyse.\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- Configuration HDFS ---\n",
    "# Chemin où sera stocké le jeu de données final, prêt pour l'analyse\n",
    "INSIGHT_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_enriched_clean.parquet\"\n",
    "\n",
    "# --- 1. Jointure d'Enrichissement ---\n",
    "# On joint toutes les observations météo (clean_final_gsod_df) aux informations de GeoJoin ID (nearest_station_df)\n",
    "# La jointure s'effectue sur l'ID de la station météo.\n",
    "\n",
    "# NOTE: Ce bloc nécessite d'avoir exécuté la Phase 6 juste avant pour que 'nearest_station_df'\n",
    "# et 'clean_final_gsod_df' soient disponibles en mémoire.\n",
    "\n",
    "final_insights_df = clean_final_gsod_df.join(\n",
    "    nearest_station_df.select(\"GEOJOIN_ID\", \n",
    "                              F.col(\"ID_STATION\").alias(\"NEAREST_STATION_ID\"), \n",
    "                              \"DISTANCE_KM\",\n",
    "                              \"LATITUDE_ZONE\", \n",
    "                              \"LONGITUDE_ZONE\"),\n",
    "    # La condition est que l'ID de la station météo DOIT correspondre à l'ID du voisin le plus proche.\n",
    "    F.col(\"ID_STATION\") == F.col(\"NEAREST_STATION_ID\"),\n",
    "    \"inner\" \n",
    ").drop(\"NEAREST_STATION_ID\") # On retire cette colonne après la jointure\n",
    "\n",
    "\n",
    "# --- 2. Sélection et Ordre Final ---\n",
    "# Crée le schéma final pour l'analyse, en plaçant les clés d'analyse en tête\n",
    "final_insights_df = final_insights_df.select(\n",
    "    \"GEOJOIN_ID\", \n",
    "    \"DATE_OBSERVATION\", \n",
    "    # Mesures Météo\n",
    "    \"TEMP_MOYENNE_F\", \n",
    "    \"PRECIPITATION_POUCES\", \n",
    "    \"VITESSE_VENT_NOEUDS\", \n",
    "    \"PHENOMENES\",\n",
    "    # Méta-données de jointure\n",
    "    \"ID_STATION\", \n",
    "    \"DISTANCE_KM\",\n",
    "    \"LATITUDE_ZONE\", \n",
    "    \"LONGITUDE_ZONE\" # Coordonnées de la zone (utile pour la cartographie)\n",
    ")\n",
    "\n",
    "print(\"\\nStructure du DataFrame Final (Météo attribuée à chaque zone de qualité de l'air) :\")\n",
    "final_insights_df.printSchema()\n",
    "final_insights_df.show(5)\n",
    "\n",
    "# --- 3. Persistance de la Couche Insight sur HDFS ---\n",
    "print(f\"\\nSauvegarde du DataFrame final Nettoyé/Enrichi (Couche Insight) dans : {INSIGHT_OUTPUT_PATH}...\")\n",
    "# Sauvegarde au format Parquet\n",
    "final_insights_df.write.mode(\"overwrite\").parquet(INSIGHT_OUTPUT_PATH)\n",
    "print(\"✅ Jointure finale et persistance terminées. Le jeu de données météo est prêt pour l'analyse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42c280-cadc-4428-89f5-4888d8c3f821",
   "metadata": {},
   "source": [
    "Phase 8 (A) : Téléchargement et Nettoyage du JSON Socrata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958651e3-8c24-4c8d-9bd4-53aadc061f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Téléchargement du JSON Socrata depuis l'API de NYC...\n",
      "✅ Fichier JSON brut sauvegardé et nettoyé structurellement à : /home/jovyan/work/data/air_quality/nyc_air_quality_raw.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configuration des Chemins ---\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/air_quality\"\n",
    "LOCAL_JSON_PATH = os.path.join(LOCAL_BASE_DIR, \"nyc_air_quality_raw.json\")\n",
    "AIR_QUALITY_URL = \"https://data.cityofnewyork.us/api/views/c3uy-2p5r/rows.json?accessType=DOWNLOAD\"\n",
    "\n",
    "# Crée le répertoire local si nécessaire\n",
    "os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
    "\n",
    "# --- 1. Téléchargement et Nettoyage de la structure JSON Socrata ---\n",
    "print(f\"⬇️ Téléchargement du JSON Socrata depuis l'API de NYC...\")\n",
    "try:\n",
    "    response = requests.get(AIR_QUALITY_URL, timeout=300) # Timeout de 5 minutes\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    # La clé 'data' contient le tableau des enregistrements bruts que Spark doit lire.\n",
    "    raw_records = data.get('data', [])\n",
    "\n",
    "    if not raw_records:\n",
    "        print(\"❌ Erreur : La clé 'data' est vide dans le JSON téléchargé. Arrêt du processus.\")\n",
    "        exit()\n",
    "    \n",
    "    # Écriture du tableau de données brutes SEULEMENT dans le nouveau fichier JSON.\n",
    "    # Ceci est essentiel pour que le RDD/toDF fonctionne correctement.\n",
    "    with open(LOCAL_JSON_PATH, 'w') as f:\n",
    "        json.dump(raw_records, f)\n",
    "\n",
    "    print(f\"✅ Fichier JSON brut sauvegardé et nettoyé structurellement à : {LOCAL_JSON_PATH}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors du téléchargement/nettoyage : {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1388c06-a297-40ce-9bc2-712369dbc6bf",
   "metadata": {},
   "source": [
    "Phase 8 (B) : Ingestion, Normalisation et Pivotage de la Qualité de l'Air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebae8573-6836-4380-bcb9-efe730824991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualité de l'air : 18862 enregistrements bruts lus.\n",
      "root\n",
      " |-- col_0: string (nullable = true)\n",
      " |-- col_1: string (nullable = true)\n",
      " |-- col_2: string (nullable = true)\n",
      " |-- col_3: string (nullable = true)\n",
      " |-- col_4: string (nullable = true)\n",
      " |-- col_5: string (nullable = true)\n",
      " |-- col_6: string (nullable = true)\n",
      " |-- col_7: string (nullable = true)\n",
      " |-- col_8: string (nullable = true)\n",
      " |-- col_9: string (nullable = true)\n",
      " |-- NOM_POLLUANT: string (nullable = true)\n",
      " |-- col_11: string (nullable = true)\n",
      " |-- col_12: string (nullable = true)\n",
      " |-- col_13: string (nullable = true)\n",
      " |-- GEOJOIN_ID_BRUT: string (nullable = true)\n",
      " |-- col_15: string (nullable = true)\n",
      " |-- col_16: string (nullable = true)\n",
      " |-- DATE_MESURE_BRUTE: string (nullable = true)\n",
      " |-- VALEUR_MESURE_BRUTE: string (nullable = true)\n",
      " |-- col_19: string (nullable = true)\n",
      "\n",
      "\n",
      "Structure pivotée des données de qualité de l'air (prête pour la jointure finale) :\n",
      "root\n",
      " |-- GEOJOIN_ID: string (nullable = true)\n",
      " |-- DATE_OBSERVATION: integer (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_MOYENNE: double (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_cars_MOYENNE: double (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_trucks_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_emergency_department_visits_due_to_PM2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_emergency_departments_visits_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_hospitalizations_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_NOx_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_PM2_5_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_SO2_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Cardiac_and_respiratory_deaths_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Cardiovascular_hospitalizations_due_to_PM2_5_age_40+_MOYENNE: double (nullable = true)\n",
      " |-- QA_Deaths_due_to_PM2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Fine_particles_PM_2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Nitrogen_dioxide_NO2_MOYENNE: double (nullable = true)\n",
      " |-- QA_Outdoor_Air_Toxics_-_Benzene_MOYENNE: double (nullable = true)\n",
      " |-- QA_Outdoor_Air_Toxics_-_Formaldehyde_MOYENNE: double (nullable = true)\n",
      " |-- QA_Ozone_O3_MOYENNE: double (nullable = true)\n",
      " |-- QA_Respiratory_hospitalizations_due_to_PM2_5_age_20+_MOYENNE: double (nullable = true)\n",
      "\n",
      "+----------+----------------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+\n",
      "|GEOJOIN_ID|DATE_OBSERVATION|QA_Annual_vehicle_miles_traveled_MOYENNE|QA_Annual_vehicle_miles_traveled_cars_MOYENNE|QA_Annual_vehicle_miles_traveled_trucks_MOYENNE|QA_Asthma_emergency_department_visits_due_to_PM2_5_MOYENNE|QA_Asthma_emergency_departments_visits_due_to_Ozone_MOYENNE|QA_Asthma_hospitalizations_due_to_Ozone_MOYENNE|QA_Boiler_Emissions-_Total_NOx_Emissions_MOYENNE|QA_Boiler_Emissions-_Total_PM2_5_Emissions_MOYENNE|QA_Boiler_Emissions-_Total_SO2_Emissions_MOYENNE|QA_Cardiac_and_respiratory_deaths_due_to_Ozone_MOYENNE|QA_Cardiovascular_hospitalizations_due_to_PM2_5_age_40+_MOYENNE|QA_Deaths_due_to_PM2_5_MOYENNE|QA_Fine_particles_PM_2_5_MOYENNE|QA_Nitrogen_dioxide_NO2_MOYENNE|QA_Outdoor_Air_Toxics_-_Benzene_MOYENNE|QA_Outdoor_Air_Toxics_-_Formaldehyde_MOYENNE|QA_Ozone_O3_MOYENNE|QA_Respiratory_hospitalizations_due_to_PM2_5_age_20+_MOYENNE|\n",
      "+----------+----------------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+\n",
      "|       503|        20210101|                                    NULL|                                         NULL|                                           NULL|                                                      NULL|                                                       NULL|                                           NULL|                                            NULL|                                              NULL|                                            NULL|                                                  NULL|                                                           NULL|                          NULL|                    6.1042113675|                    12.18993265|                                   NULL|                                        NULL|               NULL|                                                        NULL|\n",
      "| 105106107|        20210101|                                    NULL|                                         NULL|                                           NULL|                                                      NULL|                                                       NULL|                                           NULL|                                            NULL|                                              NULL|                                            NULL|                                                  NULL|                                                           NULL|                          NULL|                     6.826255998|                    18.11753712|                                   NULL|                                        NULL|               NULL|                                                        NULL|\n",
      "|       503|        20120601|                                    NULL|                                         NULL|                                           NULL|                                                      NULL|                                                       NULL|                                           NULL|                                            NULL|                                              NULL|                                            NULL|                                                  NULL|                                                           NULL|                          NULL|                            9.96|                          9.905|                                   NULL|                                        NULL|              34.01|                                                        NULL|\n",
      "|       403|        20221201|                                    NULL|                                         NULL|                                           NULL|                                                      NULL|                                                       NULL|                                           NULL|                                            NULL|                                              NULL|                                            NULL|                                                  NULL|                                                           NULL|                          NULL|               6.305303207000001|             19.574104493333333|                                   NULL|                                        NULL|               NULL|                                                        NULL|\n",
      "|       501|        20090601|                                    NULL|                                         NULL|                                           NULL|                                                      NULL|                                                       NULL|                                           NULL|                                            NULL|                                              NULL|                                            NULL|                                                  NULL|                                                           NULL|                          NULL|                          10.665|                          17.82|                                   NULL|                                        NULL| 24.645000000000003|                                                        NULL|\n",
      "+----------+----------------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "✅ Qualité de l'air pivotée et persistée sur HDFS à : hdfs://namenode:9000/user/mathis/datalake/nyc_air_quality_pivot.parquet\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- Configurations des Chemins ---\n",
    "LOCAL_BASE_DIR = \"/home/jovyan/work/data/air_quality\"\n",
    "LOCAL_JSON_PATH = os.path.join(LOCAL_BASE_DIR, \"nyc_air_quality_raw.json\")\n",
    "AIR_QUALITY_URL = \"https://data.cityofnewyork.us/api/views/c3uy-2p5r/rows.json?accessType=DOWNLOAD\"\n",
    "AIR_QUALITY_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/nyc_air_quality_pivot.parquet\"\n",
    "\n",
    "# Crée le répertoire local si nécessaire\n",
    "os.makedirs(LOCAL_BASE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- 1. Lecture du tableau JSON en mémoire Python (Assumée réussie) ---\n",
    "# Ce bloc dépend du succès de l'étape de téléchargement et de nettoyage du JSON brut.\n",
    "with open(LOCAL_JSON_PATH, 'r') as f:\n",
    "    raw_socrata_array = json.load(f)\n",
    "\n",
    "# --- 2. Définition du Schéma RDD/toDF : Correction du format Socrata ---\n",
    "\n",
    "# Mappage des index Socrata confirmés (Index dans le tableau data)\n",
    "column_names = {\n",
    "    17: \"DATE_MESURE_BRUTE\",         # Index 17 -> Start_Date\n",
    "    14: \"GEOJOIN_ID_BRUT\",           # Index 14 -> Geo Join ID\n",
    "    10: \"NOM_POLLUANT\",              # Index 10 -> Name (Polluant)\n",
    "    18: \"VALEUR_MESURE_BRUTE\"        # Index 18 -> Data Value\n",
    "}\n",
    "\n",
    "# Définir tous les noms de colonnes jusqu'à l'indice max (19)\n",
    "# La correction est ici : range(20) pour obtenir 20 colonnes (0 à 19)\n",
    "col_labels = [f\"col_{i}\" for i in range(20)] # ANCIEN: range(19)\n",
    "\n",
    "# Mappage des indices Socrata confirmés (inchangé, ils sont tous < 20)\n",
    "column_names = {\n",
    "    17: \"DATE_MESURE_BRUTE\",\n",
    "    14: \"GEOJOIN_ID_BRUT\",\n",
    "    10: \"NOM_POLLUANT\",\n",
    "    18: \"VALEUR_MESURE_BRUTE\"\n",
    "}\n",
    "\n",
    "# (Le reste du mappage est inchangé)\n",
    "for index, name in column_names.items():\n",
    "    col_labels[index] = name\n",
    "\n",
    "# CRÉATION DU SCHÉMA STRUCTURÉ (Tous en StringType)\n",
    "target_schema = StructType([\n",
    "    StructField(name, StringType(), True) for name in col_labels\n",
    "])\n",
    "\n",
    "# Création du DataFrame Spark à partir du RDD avec le schéma explicite\n",
    "air_quality_data_raw = spark.sparkContext.parallelize(raw_socrata_array).toDF(target_schema)\n",
    "\n",
    "print(f\"Qualité de l'air : {air_quality_data_raw.count()} enregistrements bruts lus.\")\n",
    "air_quality_data_raw.printSchema()\n",
    "\n",
    "\n",
    "# --- 3. Sélection et Nettoyage des Colonnes (ETL) ---\n",
    "\n",
    "air_quality_df = air_quality_data_raw.select(\n",
    "    \"DATE_MESURE_BRUTE\",\n",
    "    \"GEOJOIN_ID_BRUT\",\n",
    "    \"NOM_POLLUANT\",\n",
    "    \"VALEUR_MESURE_BRUTE\"\n",
    ")\n",
    "\n",
    "air_quality_clean = air_quality_df.filter(\n",
    "    F.col(\"VALEUR_MESURE_BRUTE\").isNotNull()\n",
    ").withColumn(\n",
    "    # Conversion de la valeur de mesure en Double\n",
    "    \"VALEUR_MESURE\", F.col(\"VALEUR_MESURE_BRUTE\").cast(DoubleType())\n",
    ").withColumn(\n",
    "    # GeoJoin ID en String pour la jointure\n",
    "    \"GEOJOIN_ID\", F.col(\"GEOJOIN_ID_BRUT\").cast(StringType())\n",
    ").withColumn(\n",
    "    # Conversion de la date (YYYY-MM-DD...) en format YYYYMMDD entier pour la jointure avec NOAA.\n",
    "    \"DATE_OBSERVATION\", \n",
    "    F.regexp_replace(F.substring(F.col(\"DATE_MESURE_BRUTE\"), 1, 10), \"-\", \"\").cast(IntegerType())\n",
    ").drop(\"DATE_MESURE_BRUTE\", \"VALEUR_MESURE_BRUTE\", \"GEOJOIN_ID_BRUT\")\n",
    "\n",
    "# --- 4. Croisement (Pivot) des Données de Qualité de l'Air ---\n",
    "# Transforme les lignes de polluants en colonnes\n",
    "air_quality_pivot = air_quality_clean.groupBy(\"GEOJOIN_ID\", \"DATE_OBSERVATION\").pivot(\"NOM_POLLUANT\").agg(\n",
    "    F.mean(\"VALEUR_MESURE\")\n",
    ")\n",
    "\n",
    "# Renommage des colonnes pivotées pour la clarté\n",
    "for col_name in air_quality_pivot.columns:\n",
    "    if col_name not in [\"GEOJOIN_ID\", \"DATE_OBSERVATION\"]:\n",
    "        # Nettoyage du nom du polluant\n",
    "        new_col_name = f\"QA_{col_name.replace(' ', '_').replace('.', '_').replace('(', '').replace(')', '').replace('/', '_')}_MOYENNE\"\n",
    "        air_quality_pivot = air_quality_pivot.withColumnRenamed(col_name, new_col_name)\n",
    "\n",
    "print(\"\\nStructure pivotée des données de qualité de l'air (prête pour la jointure finale) :\")\n",
    "air_quality_pivot.printSchema()\n",
    "air_quality_pivot.show(5)\n",
    "\n",
    "# --- 5. Persistance sur HDFS ---\n",
    "air_quality_pivot.write.mode(\"overwrite\").parquet(AIR_QUALITY_OUTPUT_PATH)\n",
    "print(f\"✅ Qualité de l'air pivotée et persistée sur HDFS à : {AIR_QUALITY_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57250055-83e0-4979-bf26-e8c0ef230c29",
   "metadata": {},
   "source": [
    "Phase 9 : Jointure Finale et Persistance du Jeu de Données Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f8a8014-a688-46a2-9996-6a3fb9a1e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structure du Jeu de Données Final (Météo + Qualité de l'Air) :\n",
      "root\n",
      " |-- GEOJOIN_ID: string (nullable = true)\n",
      " |-- DATE_OBSERVATION: string (nullable = true)\n",
      " |-- TEMP_MOYENNE_F: double (nullable = true)\n",
      " |-- PRECIPITATION_POUCES: double (nullable = true)\n",
      " |-- VITESSE_VENT_NOEUDS: double (nullable = true)\n",
      " |-- PHENOMENES: string (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_MOYENNE: double (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_cars_MOYENNE: double (nullable = true)\n",
      " |-- QA_Annual_vehicle_miles_traveled_trucks_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_emergency_department_visits_due_to_PM2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_emergency_departments_visits_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Asthma_hospitalizations_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_NOx_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_PM2_5_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Boiler_Emissions-_Total_SO2_Emissions_MOYENNE: double (nullable = true)\n",
      " |-- QA_Cardiac_and_respiratory_deaths_due_to_Ozone_MOYENNE: double (nullable = true)\n",
      " |-- QA_Cardiovascular_hospitalizations_due_to_PM2_5_age_40+_MOYENNE: double (nullable = true)\n",
      " |-- QA_Deaths_due_to_PM2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Fine_particles_PM_2_5_MOYENNE: double (nullable = true)\n",
      " |-- QA_Nitrogen_dioxide_NO2_MOYENNE: double (nullable = true)\n",
      " |-- QA_Outdoor_Air_Toxics_-_Benzene_MOYENNE: double (nullable = true)\n",
      " |-- QA_Outdoor_Air_Toxics_-_Formaldehyde_MOYENNE: double (nullable = true)\n",
      " |-- QA_Ozone_O3_MOYENNE: double (nullable = true)\n",
      " |-- QA_Respiratory_hospitalizations_due_to_PM2_5_age_20+_MOYENNE: double (nullable = true)\n",
      " |-- DISTANCE_STATION_KM: double (nullable = true)\n",
      " |-- ID_STATION: string (nullable = true)\n",
      " |-- LATITUDE_ZONE: double (nullable = true)\n",
      " |-- LONGITUDE_ZONE: double (nullable = true)\n",
      "\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+-------------------+----------+-------------+--------------+\n",
      "|GEOJOIN_ID|DATE_OBSERVATION|TEMP_MOYENNE_F|PRECIPITATION_POUCES|VITESSE_VENT_NOEUDS|PHENOMENES|QA_Annual_vehicle_miles_traveled_MOYENNE|QA_Annual_vehicle_miles_traveled_cars_MOYENNE|QA_Annual_vehicle_miles_traveled_trucks_MOYENNE|QA_Asthma_emergency_department_visits_due_to_PM2_5_MOYENNE|QA_Asthma_emergency_departments_visits_due_to_Ozone_MOYENNE|QA_Asthma_hospitalizations_due_to_Ozone_MOYENNE|QA_Boiler_Emissions-_Total_NOx_Emissions_MOYENNE|QA_Boiler_Emissions-_Total_PM2_5_Emissions_MOYENNE|QA_Boiler_Emissions-_Total_SO2_Emissions_MOYENNE|QA_Cardiac_and_respiratory_deaths_due_to_Ozone_MOYENNE|QA_Cardiovascular_hospitalizations_due_to_PM2_5_age_40+_MOYENNE|QA_Deaths_due_to_PM2_5_MOYENNE|QA_Fine_particles_PM_2_5_MOYENNE|QA_Nitrogen_dioxide_NO2_MOYENNE|QA_Outdoor_Air_Toxics_-_Benzene_MOYENNE|QA_Outdoor_Air_Toxics_-_Formaldehyde_MOYENNE|QA_Ozone_O3_MOYENNE|QA_Respiratory_hospitalizations_due_to_PM2_5_age_20+_MOYENNE|DISTANCE_STATION_KM|ID_STATION|LATITUDE_ZONE|LONGITUDE_ZONE|\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+-------------------+----------+-------------+--------------+\n",
      "+----------+----------------+--------------+--------------------+-------------------+----------+----------------------------------------+---------------------------------------------+-----------------------------------------------+----------------------------------------------------------+-----------------------------------------------------------+-----------------------------------------------+------------------------------------------------+--------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------------------+------------------------------+--------------------------------+-------------------------------+---------------------------------------+--------------------------------------------+-------------------+------------------------------------------------------------+-------------------+----------+-------------+--------------+\n",
      "\n",
      "\n",
      "✅ Jeu de données final (Météo + Qualité de l'Air) créé et persisté à : hdfs://namenode:9000/user/mathis/datalake/nyc_final_air_weather_dataset.parquet\n",
      "Le pipeline d'Ingestion, Persistance et Traitement est terminé. Votre jeu de données est prêt pour l'Insight. 📊\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# --- Configuration des Chemins (Réutilisés) ---\n",
    "# Chemin des données NOAA enrichies (Couche Insight après Phase 7)\n",
    "INSIGHT_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/noaa_gsod_nyc_enriched_clean.parquet\"\n",
    "# Chemin des données Qualité de l'Air pivotées (Couche Pivot après Phase 8)\n",
    "AIR_QUALITY_OUTPUT_PATH = \"hdfs://namenode:9000/user/mathis/datalake/nyc_air_quality_pivot.parquet\"\n",
    "# Chemin du jeu de données final\n",
    "FINAL_DATASET_PATH = \"hdfs://namenode:9000/user/mathis/datalake/nyc_final_air_weather_dataset.parquet\"\n",
    "\n",
    "# --- 1. Lecture des deux sources enrichies depuis HDFS ---\n",
    "final_insights_df = spark.read.parquet(INSIGHT_OUTPUT_PATH)\n",
    "air_quality_pivot = spark.read.parquet(AIR_QUALITY_OUTPUT_PATH)\n",
    "\n",
    "# --- 2. Jointure Finale Temporelle et Géographique ---\n",
    "# La jointure se fait sur deux clés : l'ID de la zone et la Date de l'observation\n",
    "final_dataset = final_insights_df.join(\n",
    "    air_quality_pivot,\n",
    "    on=[\"GEOJOIN_ID\", \"DATE_OBSERVATION\"],\n",
    "    how=\"inner\" # Utilisation de 'inner' pour ne garder que les jours où les DEUX mesures sont présentes\n",
    ")\n",
    "\n",
    "# --- 3. Sélection Finale et Sauvegarde ---\n",
    "# Réorganisation et sélection des colonnes pour le jeu de données d'analyse finale\n",
    "final_dataset = final_dataset.select(\n",
    "    \"GEOJOIN_ID\", \n",
    "    \"DATE_OBSERVATION\", \n",
    "    # Météo\n",
    "    \"TEMP_MOYENNE_F\", \n",
    "    \"PRECIPITATION_POUCES\", \n",
    "    \"VITESSE_VENT_NOEUDS\", \n",
    "    \"PHENOMENES\",\n",
    "    # Qualité de l'Air (toutes les colonnes commençant par QA_)\n",
    "    *[col for col in final_dataset.columns if col.startswith(\"QA_\")],\n",
    "    # Métadonnées de jointure\n",
    "    F.col(\"DISTANCE_KM\").alias(\"DISTANCE_STATION_KM\"),\n",
    "    \"ID_STATION\",\n",
    "    \"LATITUDE_ZONE\", \n",
    "    \"LONGITUDE_ZONE\" \n",
    ")\n",
    "\n",
    "print(\"\\nStructure du Jeu de Données Final (Météo + Qualité de l'Air) :\")\n",
    "final_dataset.printSchema()\n",
    "final_dataset.show(5)\n",
    "\n",
    "# --- 4. Persistance Finale sur HDFS ---\n",
    "final_dataset.write.mode(\"overwrite\").parquet(FINAL_DATASET_PATH)\n",
    "\n",
    "print(f\"\\n✅ Jeu de données final (Météo + Qualité de l'Air) créé et persisté à : {FINAL_DATASET_PATH}\")\n",
    "print(\"Le pipeline d'Ingestion, Persistance et Traitement est terminé. Votre jeu de données est prêt pour l'Insight. 📊\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985c9740-407f-434f-8f3a-c2312cc3f891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
